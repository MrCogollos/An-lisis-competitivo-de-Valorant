## Import required libraries and machine learning models for this analysis

import polars as pl
import numpy as np
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import math
# ML Models
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
pysqlDraft = lambda q : sqlDraft(q,globals())

## 1. Data Loading
Map_score = pd.read_csv(r"C:\Users\bastian\Documents\VSC datos\data\Valorant\vct_2025\matches\maps_scores.csv")
stats_players= pd.read_csv(r"C:\Users\bastian\Documents\VSC datos\data\Valorant\vct_2025\matches\overview.csv")

## 2. Exploratory Analysis
# Inspect the information of both DataFrames using .info()
Map_score.info()

# Inspect the information of both DataFrames using .info()
stats_players.info()

# Some matches do not contain advanced metrics (Rating, ACS, ADR, KAST)
# due to limitations of the data source (VLR.gg), such as walkovers,
# older matches, or special tournament formats.
# These observations were not artificially imputed in order to avoid
# introducing bias into the analysis.

# For exploratory analysis, all observations were retained,
# while for predictive modeling only matches with complete
# statistics were filtered.
stats_players['stats_available'] = stats_players[
    ['Rating', 'Average Combat Score', 'Average Damage Per Round', 'Kill, Assist, Trade, Survive %']
].notna().all(axis=1)

# This allows us to obtain a clean dataset with complete data,
# which can be used to group by team, tournament, map, among others,
# and to build predictive models.
stats_completos = stats_players[stats_players['stats_available']]
stats_completos

# Create a function to determine the team that won each map
def Ganador(Map_score):
    if Map_score['Team A Score'] > Map_score['Team B Score']:
        return Map_score['Team A']
    elif Map_score['Team A Score'] < Map_score['Team B Score']:
        return Map_score['Team B']
    else:
        return 'Draw'
    
Map_score['Ganador'] = Map_score.apply(Ganador, axis=1)
Map_score

# Reset the index to avoid issues when merging columns
# from both datasets using the .merge() method
stats_completos = stats_completos.reset_index()

stats_completos = stats_completos.merge(
    Map_score[['Tournament', 'Match Name', 'Map', 'Ganador']],
    on=['Tournament', 'Match Name', 'Map'],
    how='left'
)

# Remove rows where the side ('Side') is attacker or defender,
# keeping only the final match result labeled as 'Both'
# for a more general analysis
stats_completos = stats_completos.drop(
    stats_completos[stats_completos['Side'].isin(['attack', 'defend'])].index
)

# Finally, drop the 'Side' column since the only remaining
# information it contains is 'both'
stats_completos = stats_completos.drop(columns={'Side'})

# Classify match regions into the four main regions,
# grouping international tournaments such as Masters and Champions
regiones = {
    'China': 'CHINA',
    'EMEA': 'EMEA',
    'Pacific': 'PACIFIC',
    'Americas': 'AMERICAS',
    'Champions': 'INTERNATIONAL',
    'Masters Toronto': 'INTERNATIONAL',
    'Masters Bangkok': 'INTERNATIONAL',
}

def Region(tournament):
    for keyword, region in regiones.items():
        if keyword.lower() in tournament.lower():
            return region
    return 'INTERNATIONAL'

stats_completos['Region'] = stats_completos['Tournament'].apply(Region)

# Rename column names for better visual readability,
# since the meaning of these metrics is already known
stats_completos = stats_completos.rename(
    columns={
        'Kill, Assist, Trade, Survive %': 'KAST%',
        'Headshot %': 'HS%',
        'Average Damage Per Round': 'ADR',
        'Average Combat Score': 'ACS',
        'First Kills': 'FK',
        'First Deaths': 'FD',
        'Kills - Deaths (FKD)': 'FKD',
        'Kills - Deaths (KD)': 'KD'
    }
)

# Reset the DataFrames to avoid compatibility issues when using .merge()
stats_completos = stats_completos.reset_index(drop=True)
Map_score = Map_score.reset_index(drop=True)

# Reset the DataFrames to avoid compatibility issues when using .merge()
stats_completos = stats_completos.reset_index(drop=True)
Map_score = Map_score.reset_index(drop=True)

# Identify the opponent team for each player by grouping
# matches at the map level
def get_opponent(group):
    # Create a safe copy to avoid SettingWithCopyWarning
    group = group.copy()

    teams = group['Team'].unique()

    # Case: exactly 2 teams (normal scenario)
    if len(teams) == 2:
        team_map = {teams[0]: teams[1], teams[1]: teams[0]}

    # Edge case: only 1 team (data issue)
    elif len(teams) == 1:
        team_map = {teams[0]: None}

    # Edge case: more than 2 teams (grouping issue)
    else:
        team_map = {t: None for t in teams}

    group['Opponent'] = group['Team'].map(team_map)

    return group

stats_completos = (
    stats_completos
    .groupby(['Tournament', 'Match Name', 'Map'], group_keys=False)
    .apply(get_opponent)
)

# Classify columns into categorical and numerical variables
letras = {
    'Tournament', 'Stage', 'Match Type', 'Match Name', 'Map',
    'Player', 'Team', 'Agents', 'Opponent', 'Region',
    'Ganador', 'stats_disponibles'
}
numeros = {'Rating', 'ACS', 'KD', 'KAST%', 'ADR', 'HS%', 'FK', 'FD', 'FKD'}

# Since HS% and KAST% are percentage-based metrics,
# convert their values to float for numerical analysis
def clean_percent_value(x):
    if pd.isna(x):
        return None

    x = str(x).strip().replace("%", "")

    if x == "":
        return None

    return float(x)

for col in ['KAST%', 'HS%']:
    stats_completos[col] = stats_completos[col].apply(clean_percent_value)

# Compute the correlation matrix of numerical statistics
stats_completos.drop(letras, axis=1).corr()

# As observed, Rating—which represents overall round impact—
# shows strong correlation with KAST%, KD, ACS, and ADR,
# indicating that these metrics are key contributors to player performance.

# Count how many maps were played per match
map_count = (
    Map_score
    .groupby(['Tournament', 'Match Name'])
    .size()
    .reset_index(name='Map_count')
)

# Keep only matches with two or more maps to exclude showmatches
valid_matches = map_count[map_count['Map_count'] >= 2][
    ['Tournament', 'Match Name']
]

# Filter Map_score to include only valid matches
Map_score = Map_score.merge(
    valid_matches,
    on=['Tournament', 'Match Name'],
    how='inner'
)
Map_score

# Avoid metric inflation caused by aggregated map rows,
# since 'All Maps' is repeated once per player (10 times per match)
stats_completos = stats_completos[
    stats_completos['Map'] != 'All Maps'
]

# Total number of maps played per team
maps_played = pd.concat([
    Map_score[['Team A']].rename(columns={'Team A': 'Team'}),
    Map_score[['Team B']].rename(columns={'Team B': 'Team'})
])

maps_played = maps_played.value_counts().reset_index(name='Maps_played')

# Maps won by each team (without considering direct matchups)
maps_won = Map_score['Ganador'].value_counts().reset_index()
maps_won.columns = ['Team', 'Maps_won']

# Merge the previously created datasets and compute the winrate
winrate = maps_played.merge(maps_won, on='Team', how='left').fillna(0)
winrate['Winrate (%)'] = 100 * winrate['Maps_won'] / winrate['Maps_played']

winrate = winrate.sort_values('Winrate (%)', ascending=False)

# Add lost maps for a more complete and interpretable overview
winrate['Maps_lost'] = winrate['Maps_played'] - winrate['Maps_won']
winrate = winrate[['Team', 'Maps_played', 'Maps_won', 'Maps_lost', 'Winrate (%)']]

# Finally, create a bar chart for quick visual analysis.
# Alternatively, the 'winrate' table can be inspected directly
# for a more detailed team-level analysis
winrate.sort_values('Maps_won', ascending=False).set_index('Team')[['Maps_won', 'Maps_lost']].plot(
    kind='bar',
    stacked=True,
    figsize=(10, 5),
    color=['#1f77b4', '#d62728']
)

plt.title("Maps won vs lost by team")
plt.ylabel("Number of maps")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Select relevant columns to create a binary match outcome variable (1 = win, 0 = loss)
GP = (
    stats_completos[['Tournament', 'Match Name', 'Match Type', 'Team', 'Opponent', 'Ganador']]
    .drop_duplicates()
)

# Binary match-level outcome: 1 if the team won the match, 0 otherwise
GP['match_win'] = (GP['Team'] == GP['Ganador']).astype(int)

# Reset indices to ensure clean merges
stats_completos = stats_completos.reset_index(drop=True)
GP = GP.reset_index(drop=True)

# Merge match-level outcome back into the full dataset
stats_completos = stats_completos.merge(
    GP[['Tournament', 'Match Name', 'Team', 'Opponent', 'match_win']],
    on=['Tournament', 'Match Name', 'Team', 'Opponent'],
    how='left'
)

# Ensure a clean index
stats_completos = stats_completos.reset_index(drop=True)

# Remove rows without actual player statistics
stats_completos = stats_completos[stats_completos['stats_disponibles'] == True]

# Remove aggregated map entries
stats_completos = stats_completos[stats_completos['Map'] != 'All Maps']

target = 'match_win'

# ❌ Forbidden variables (data leakage / identifiers)
drop_cols = [
    'match_win',
    'Ganador',        # Contains the true match outcome
    'Match Name',     # Identifies the match
    'Player',         # Player-level identifiers (risk of memorization)
    'index'
]

X = stats_completos.drop(columns=drop_cols, errors='ignore')
y = stats_completos[target]

# Numerical features
numeric_features = [
    'Rating', 'ACS', 'Kills', 'Deaths', 'Assists',
    'KD', 'ADR', 'KAST%', 'HS%', 'FK', 'FD', 'FKD'
]

# Categorical features
categorical_features = [
    'Tournament', 'Stage', 'Match Type',
    'Map', 'Team', 'Opponent', 'Agents', 'Region'
]

# Preprocessing pipeline: scaling for numerical features
# and one-hot encoding for categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ]
)

# Group-based split to prevent match-level data leakage
gss = GroupShuffleSplit(test_size=0.2, random_state=42)

train_idx, test_idx = next(
    gss.split(X, y, groups=stats_completos['Match Name'])
)

X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

# Models to be evaluated
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=300, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=300, random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\n========== {name} ==========")

    pipe = Pipeline(steps=[
        ('preprocess', preprocessor),
        ('model', model)
    ])

    # Train the model
    pipe.fit(X_train, y_train)

    # Predictions
    y_pred = pipe.predict(X_test)
    y_prob = pipe.predict_proba(X_test)[:, 1]

    # Classification metrics
    print(classification_report(y_test, y_pred))

    # ROC-AUC score
    roc_auc = roc_auc_score(y_test, y_prob)
    results[name] = roc_auc

    # Confusion Matrix
    cm = pd.crosstab(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], '--', color='gray')
    plt.title(f'{name} - ROC Curve')
    plt.legend()
    plt.show()

# Compare model performance
results_df = pd.DataFrame.from_dict(results, orient='index', columns=['ROC_AUC'])
results_df.sort_values('ROC_AUC', ascending=False)

# The objective is to compare multiple predictive models
# and determine which one provides the best performance
# for match outcome prediction

# Finally, as observed in the previous evaluation, the Random Forest model
# provides the best predictive performance in our case. Therefore, we proceed
# to analyze feature importance to understand which variables contribute most
# to the model's decisions.

pipe = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('model', RandomForestClassifier(n_estimators=300, random_state=42))
])

# Train the Random Forest model
pipe.fit(X_train, y_train)

rf_model = pipe.named_steps['model']
preprocess = pipe.named_steps['preprocess']

# Numerical feature names (remain unchanged)
num_features = numeric_features

# One-hot encoded categorical feature names
cat_features = preprocess.named_transformers_['cat'].get_feature_names_out(categorical_features)

# Final feature set used by the model
feature_names = np.concatenate([num_features, cat_features])

# Extract feature importances from the trained Random Forest
importances = rf_model.feature_importances_

feat_imp = (
    pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    })
    .sort_values('Importance', ascending=False)
)

# Plot the top 20 most important features
plt.figure(figsize=(12, 6))
sns.barplot(
    data=feat_imp.head(20),
    x='Importance',
    y='Feature',
    palette='viridis'
)
plt.title('Random Forest - Top 20 Feature Importances')
plt.tight_layout()
plt.show()

# Finally, we predict the probability of winning a map between two teams.
# The 'Team' variable represents the team of interest, and the returned value
# corresponds to the model-estimated probability of victory.

def predict_match_with_history(pipe, team, opponent, map_name):
    # Retrieve historical average statistics for the given team and map
    row_stats = team_map_avg[
        (team_map_avg['Team'] == team) &
        (team_map_avg['Map'] == map_name)
    ]

    # If no historical data is available, the prediction cannot be computed
    if row_stats.empty:
        raise ValueError('No historical data available for this team and map')

    r = row_stats.iloc[0]

    # Construct a single-row dataframe matching the model input schema
    row = pd.DataFrame([{
        'Tournament': 'VCT 2025: Americas Stage 1',
        'Stage': 'Playoffs',
        'Match Type': 'Elimination',
        'Map': map_name,
        'Team': team,
        'Opponent': opponent,
        'Agents': 'unknown',
        'Rating': r['Rating'],
        'ACS': r['ACS'],
        'Kills': r['Kills'],
        'Deaths': r['Deaths'],
        'Assists': r['Assists'],
        'KD': r['KD'],
        'ADR': r['ADR'],
        'KAST%': r['KAST%'],
        'HS%': r['HS%'],
        'FK': r['FK'],
        'FD': r['FD'],
        'FKD': r['FKD'],
        'stats_disponibles': True,
        'Region': 'AMERICAS'
    }])

    # Return the predicted probability of winning the map
    return pipe.predict_proba(row)[0][1]


# Example prediction
p = predict_match_with_history(pipe, 'G2 Esports', 'Sentinels', 'Ascent')
print(f'Probability of G2 Esports winning: {p:.2%}')

result: Probability of G2 Esports to win: 60.34%

